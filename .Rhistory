##############################################################
#' Simulates data from the aforementioned model
#'
#' This function can be used to simulate observations from the aforementioned model, if
#' \eqn{G(\cdot)} is chosen as a finite Gaussian mixture. It returns the true local
#' false discovery rates which determine the optimal multiple testing procedure.
#' @param n Number of z-scores to be generated.
#' @param x \eqn{n\times}p data matrix. Do not add an additional column of \eqn{1's}.
#' @param sx The vector of coefficients for the logistic function. The first entry will be considered as the intercept term by default. Requires compatibility with x and df. See \strong{Details}.
#' @param atoms The vector of means for each component of the mixing distribution.
#' @param probs The probability vector for the mixing distribution.
#' @param variances The vector of variances for each component of the mixing distribution. Requires compatibility with atoms and probs. See \strong{Details}.
#' @param df A positive real number to denote the degrees of freedom for a basis spline expansion on each column of the data matrix x. The default value is 0, which does not change x. See the bs() function in the \emph{splines} package.
#' @details Given \eqn{X=x}, a Bernoulli\eqn{(\pi^*(x))} sample is drawn. If the outcome is
#' 1 (0), a z-score is drawn from \eqn{\phi_1(\cdot)} \eqn{(\phi(\cdot))}. All the observations
#' corresponding to a Bernoulli outcome 1 (0) is termed as \emph{non-null observations}
#' (\emph{null observations}).\cr
#' The length of sx should be 1 more than the number of columns of the data matrix x (if df = ,
#' or the transformed data matrix after a basis spline expansion (if df > 0). For example, if x
#' is a \eqn{n\times 2} matrix and df=3, then the transformed data matrix has dimensions
#' \eqn{n\times 6} and accordingly sx must have length 7.\cr
#' The vectors atoms, probs and variances must have the same length.
#' @return The output is a list with the following entries:
#' @return \item{y}{The vector of simulated z-scores.}
#' @return \item{x}{The input data matrix.}
#' @return \item{xs}{The data matrix after the columnwise basis spline expansion. Same as x if df=0.}
#' @return \item{pix}{The vector of signal proportions.}
#' @return \item{f0y}{The vector of standard Gaussian densities evaluated at simulated z-scores.}
#' @return \item{f1y}{The vector of signal densities evaluated at simulated z-scores.}
#' @return \item{den}{The vector of conditional densities evaluated at simulated z-scores.}
#' @return \item{localfdr}{The vector of local false discovery rates evaluated at simulated z-scores. Note that the local FDR can be interpreted as one minus the posterior probability that a given observation is non-null.}
#' @return \item{ll}{The average conditional log-likelihood.}
#' @return \item{nnind}{The indices corresponding to non-null observations.}
#' @examples
#' require(NPMLEmix)
#'
#' #--- Using makedata() without a basis spline expansion ---#
#' x=cbind(runif(1000),runif(1000))
#' n=1000
#' atoms=c(-2,0,2)
#' probs=c(0.48,0.04,0.48)
#' variances=c(1,16,1)
#' sx=c(-3,1.5,1.5)
#' makedata(n,x,sx,atoms,probs,variances)
#'
#' #--- Using makedata() with a basis spline expansion on each column ---#
#'
#' x=cbind(runif(100),runif(100))
#' n=100
#' atoms=c(0,1)
#' probs=c(0.4,0.6)
#' variances=c(1,1)
#' sx=c(0,-1,1,1,-1,-1,1)
#' makedata(n,x,sx,atoms,probs,variances,df=3)
#'
#' @references Basu, P., Cai, T.T., Das, K. and Sun, W., 2018. Weighted false discovery rate control in large-scale multiple testing. Journal of the American Statistical Association, 113(523), pp.1172-1183.
#' @references Scott, J.G., Kelly, R.C., Smith, M.A., Zhou, P. and Kass, R.E., 2015. False discovery rate regression: an application to neural synchrony detection in primary visual cortex. Journal of the American Statistical Association, 110(510), pp.459-471.
#' @export
makedata = function(n, x , sx, atoms, probs, variances){
# constructing f1
atoms = atoms; probs = probs; variances = variances;
# checking validity of tdparams
stopifnot(length(unique(c(length(atoms), length(probs), length(variances)))) == 1)
# number of components
nc = length(atoms)
# f1 function
f1 = make_density(atoms, probs, variances)
# here we generate data
####################################################################
# starting data generation process
xs = x
# checking further validity
stopifnot(length(unique(c(ncol(xs), (length(sx)-1)))) == 1)
stopifnot(length(unique(c(nrow(xs), n))) == 1)
# generating pi(.)
pix = sapply(evnnprop(sx,xs), function(x) return(1/(1+exp(-x))))
# generating observations
nulls = (runif(n) >= pix);  nind = which(nulls); nnind = setdiff(1:n, nind);
# populating data
theta = array(0, dim = n)
wc = sample(nc, length(nnind), replace = TRUE, prob = probs)
theta[nnind] = atoms[wc] + rnorm(length(nnind)) * sqrt(variances[wc])
y = rnorm(n) + theta
# completed data generation process
#####################################################################
f0y = dnorm(y)
f1y = f1(y)
den = (1-pix)*f0y + pix*f1y
localfdr = (1-pix)*f0y/den
ll = mean(log(den))
return(list(y = y, x = x, xs = xs,
pix = pix,
f0y = f0y, f1y = f1y, localfdr = localfdr, ll = ll, f1 = f1,
nnind = nnind, den = den))
}
############################################################
# utility to make a density given atoms, probs and vars
############################################################
make_density = function(atoms,probs,variances){
f <- function(xx) sum(sapply(1:length(atoms), function(j)
probs[j] * dnorm(xx, atoms[j], sqrt(1 + variances[j]))))
return(Vectorize(f))
}
############################################################
# Rejection set in the multiple hypotheses testing problem
############################################################
reject_set = function(locfdr, level){
asclfdr = sort(locfdr)
cumlfdr=cummean(asclfdr)
ind=max(which(cumlfdr <= level))
rejvec=as.numeric(locfdr <= asclfdr[ind])
return(rejvec)
}
# this function is used by REBayes to construct the mids
histbin <- function(x, m = histm, eps = 1e-06, weights = weights) {
u <- seq(min(x) - eps, max(x) + eps, length = m)
xu <- findInterval(x, u)
txu <- tabulate(xu)
midu <- (u[-1] + u[-m])/2
wnz <- (txu > 0)
if (length(weights)) {
if (length(weights) == length(x))
w <- as.numeric(tapply(weights, xu, sum))
else stop("length(weights) not equal length(x)")
}
else w <- txu[wnz]/sum(txu[wnz])
list(midu = midu[wnz], w = w)
}
# let's try implementing primal formulation of kw in mosek
# this can be used to solve both the weighted and pi-constrained problems
kwprimal = function(y, pivec = 1, weights = 1, grid_len = 100){
LIMIT_THREAD = TRUE
m = grid_len
n = length(y)
# if (length(pivec) == 1) pivec = rep(pivec, n)
# if (length(weights) == 1) pivec = rep(weights, n)
atoms = seq(from = min(y), to = max(y), length.out = m)
fmat = dnorm(outer(y, atoms, "-"))
# setting up problem
# want to maximize so sense = max
# the variables which need to be optimized are (p_j, v_i)
moseko = list(sense = "max")
# the problem is stated by mosek in the form:
# sum of non-linear functions separable in arguments + linear part (c^T\pi)
# for us there is no linear part so c = 0
moseko$c = rep(0, m + n)
# monotone constraints in pi
A = rbind(cbind(fmat, -diag(n)), c(rep(c(1,0), times = c(m,n))))
# sparsify to increase speed
moseko$A = as(A, "CsparseMatrix")
# matrix of two rows
# first row (blc) is lower bound on each linear relationship defined by A
# second row (blu) is upper bound on each linear relationship defined by A
moseko$bc = rbind(blc = c(rep(0, n), 1), buc = c(rep(0, n), 1))
# box constraints: individual constraints on each pi_i
# it seems to be easier to specify these separately
# similar to above
# first row (bxl) is lower bound on each pi_i
# second row (bxu) is upper bound on each pi_i
moseko$bx = rbind(blx = rep(0, m + n), bux = c(rep(1, m), rep(Inf, n)))
# this is the interesting part, specifies the non-linear function in the objective
# the function needs to separable in pi_i,
# \sum_k f_k phi_k( g_k \pi_{j_k} + h_k)
# where k is the k-th column of the following matrix
# the number of columns is the number of non-linear functions the objective is separated into
# the k-th column is a function of the j_k variable
# the non-linearity is determined by 'type' (written as phi above)
# you can multiply phi_k by some constant: f_k
# also phi_k can be evaluated at a linear function of the variable in question: (g_k, h_k)
opro = matrix(list(), nrow = 5, ncol = n)
rownames(opro) = c("type", "j", "f", "g", "h")
opro[1, ] = rep("LOG", n)
opro[2, ] = m + 1:n
opro[3, ] = weights # coefficients outside log, in this case all 1
opro[4, ] = pivec # coefficient of v_i inside log
opro[5, ] = (1-pivec) * dnorm(y) #constants inside log
moseko$scopt = list(opro = opro)
if(LIMIT_THREAD) {moseko$iparam$num_threads <- 1}
ans <- mosek(moseko, opts = list(verbose = 1))
probs = ans$sol$itr$xx[1:m]
f1y = ans$sol$itr$xx[m + 1:n]
return(list(atoms = atoms, probs = probs, f1y = f1y, ll = mean(log(f1y))))
}
# kwprimal using weights and histogram
kwprimal_weights = function(y, weights = 1, num_atoms = 100, hist_flag = TRUE, num_hist = num_atoms){
n = length(y);
if (length(weights) == 1) {weights = rep(weights, n)}
# constructing histogram object
if(hist_flag){
histo = histbin(y, m = num_hist, weights = weights)
# the compressed dataset is midu and w
# print(names(histo))
yy = histo$midu; ww = histo$w;
} else {
yy = y; ww = weights;
}
kwo = kwprimal(yy, weights = ww, grid_len = num_atoms);
if (hist_flag){
fmat = dnorm(outer(y, kwo$atoms, "-"))
kwo$f1y = as.vector(fmat %*% kwo$probs)
kwo$ll = mean(log(kwo$f1y))
}
return(kwo)
}
lgstep = function(y, x, w, b, num_atoms, blambda, level){
f0y = dnorm(y)
# take one step of EM on data
kwo = kwprimal_weights(y, weights = w, num_atoms)
lp = lregoptim(f0y, kwo$f1y, x, b, lambda = blambda)
# compute results
den = (1 - lp$p) * f0y + lp$p * kwo$f1y; ll = mean(log(den)); localfdr = (1 - lp$p) * f0y/(den);
rejset = reject_set(localfdr, level)
return(list(p = lp$p, b = lp$b, f1y = kwo$f1y, kwo=kwo,
localfdr = localfdr, den = den, ll = ll, rejset = rejset))
}
# solve logistic problem given alternate density (AM version)
lregoptim = function(f0y, f1y, x, binit, lambda = 1e-2/length(f0y)){
# by default this provides a l2-regularization
# equivalent to gaussian prior N(0,100) on all parameters
# lambda = c(1e-2/length(f0y), rep(lambda, length(binit)-1))
# defining objective function
llobj = function(bb, f0y, f1y, x, lambda){
pivec = as.vector(1/(1 + exp(-x %*% bb)))
- mean(log(pivec*f1y + (1-pivec)*f0y)) + sum(lambda*bb^2)
}
# defining first gradient
llfirst = function(bb, f0y, f1y, x, lambda){
pivec = as.vector(1/(1 + exp(-x %*% bb)))
wts = (f1y - f0y)/(pivec*f1y + (1-pivec)*f0y) * pivec * (1-pivec)
- apply(x, 2, function(vec) mean(vec * wts)) + 2*lambda*bb
}
optimres = optim(par = binit, fn = llobj, gr = llfirst,
f0y = f0y, f1y = f1y, x = x, lambda = lambda,
method = 'BFGS')
b = optimres$par
p = as.vector(1/(1 + exp(-x %*% b)))
return(list(b = b, p = p))
}
marg1 = function(y, x, blambda = 1e-6/length(y), level = 0.05){
# x=cbind(1, datax)
mt = abs(y) - mean(abs(rnorm(1e4)));
pi0grid = seq(from = 0.01, to = 0.49, by = 0.01);
verbose=FALSE;
grid_len = max(100, round(sqrt(length(y))));
n = length(y); f0y = dnorm(y);
kwm = kwprimal_weights(y, num_atoms = grid_len);
fmy = kwm$f1y;
m1step = function (pt) {
weights = pmax(pmin(1 - (1-pt)*f0y/fmy, 0.99), 0.01)
muinit = mean(mt)/pt
binit = lm(mosaic::logit(pmax(pmin(mt/muinit, 0.9), 0.1)) ~ 0 + x)$coefficients;
robj = lgstep(y, x, weights, binit, grid_len, blambda, level)
robj$pi0 = pt
return(robj)
}
if (verbose) {res = pblapply(pi0grid, m1step)}
else {res = lapply(pi0grid, m1step)}
ll_list = sapply(res, function (robj) robj$ll)
bi = which.max(ll_list)
robj = res[[bi]]
robj$ll_list = ll_list
return(robj)
}
# solve logistic problem given alternate density (EM version)
lregem = function(weights, x, binit, lambda = 1e-2/length(weights)){
# by default this provides a l2-regularization
# equivalent to gaussian prior N(0,100) on all parameters
# lambda = c(1e-2/length(f0y), rep(lambda, length(binit)-1))
# defining objective function
llobj = function(bb, weights, x, lambda){
pivec = as.vector(1/(1 + exp(-x %*% bb)))
- mean(weights * log(pivec) + (1-weights) * log(1-pivec)) + sum(lambda*bb^2)
}
# defining first gradient
llfirst = function(bb, weights, x, lambda){
pivec = as.vector(1/(1 + exp(-x %*% bb)))
tt = pivec * (1-pivec) * (weights/pivec - (1-weights)/(1-pivec))
- apply(x, 2, function(vec) mean(vec * tt)) + 2*lambda*bb
}
optimres = optim(par = binit, fn = llobj, gr = llfirst,
weights = weights, x = x, lambda = lambda,
method = 'BFGS')
b = optimres$par
p = as.vector(1/(1 + exp(-x %*% b)))
return(list(b = b, p = p))
}
# EM
lgem = function(y, x,
weights, binit,
grid_len = 3*max(100, round(sqrt(length(y)))), histFlag = TRUE, timed = 60,
maxit = 600, tol = 1e-6,
blambda = 1e-6/length(y), level){
st = proc.time()['elapsed']
n = length(y); f0y = dnorm(y);
# initial values, may need to revisit this
lp = NULL; lp$b = binit; lp$p = as.vector(1/(1 + exp(-x %*% lp$b)));
# starting iterations
convFlag = 1; ll = NULL; lp_list = NULL; kw_list = NULL; time_list = NULL; itcount = 1; err_list = NULL;
# if(verbose) pb = txtProgressBar(max = maxit+1, style = 3)
while(convFlag){
kwo = kwprimal_weights(y, weights = weights, num_atoms = grid_len, hist_flag = histFlag)
lp = lregem(weights, x, lp$b, lambda = blambda)
weights_new = lp$p*kwo$f1y/((1-lp$p)*f0y + lp$p*kwo$f1y)
iter_error = rmse(weights_new, weights); err_list = append(err_list ,iter_error);
lp_list = append(lp_list, list(lp)); kw_list = append(kw_list, list(kwo));
ll = append(ll, mean(log((1-lp$p)*f0y + lp$p*kwo$f1y)))
ct = proc.time()['elapsed'] - st
time_list = append(time_list, ct)
convFlag = (ct <= timed) & (itcount < maxit) & (iter_error > tol)
weights = weights_new
itcount = itcount + 1
}
localfdr = 1 - weights
rejset = reject_set(localfdr, level)
den = (1-lp$p)*f0y + lp$p*kwo$f1y
return(list(atoms = kwo$atoms, probs = kwo$probs, f1y = kwo$f1y,
b = lp$b, p = lp$p,
f0y = f0y, den = den,
localfdr = localfdr, rejset = rejset,
ll = ll,  lp_list = lp_list, kw_list = kw_list,
err_list = err_list, time_list = time_list,
runtime = proc.time()['elapsed'] - st))
}
# marginal method 2, using a grid of potential values of pi0
# remember to use a good choice of mt (marginal transform)
marg2 = function(y, x, nlslambda = 1e-6/length(y), level = 0.05){
# for an user given sequence pi0 (overall non-null proportion)
# solve non-linear least squares on marginal_transform to get initial value of beta
# x=cbind(1,x);
pi0grid = seq(from = 0.01, to = 0.49, by = 0.01);
mt = abs(y) - mean(abs(rnorm(1e4)));
verbose = FALSE;
solvef1 = TRUE;
n = length(y); f0y = dnorm(y);
mugrid = mean(mt)/pi0grid;
computefn = function (mu) {
# this function uses non-linear least squares to solve for beta for a fixed value of mu
binit = lm(mosaic::logit(pmax(pmin(mt/mu, 0.9), 0.1)) ~ 0 + x)$coefficients
nlso = m2boptim(mt/mu, x, binit, nlslambda)
er = mean((mt - mu * nlso$p)^2)
return(list(nlso = nlso, er = er, mu = mu, pi0 = mean(mt)/mu))
}
if (verbose) {
res = pblapply(mugrid, computefn)
} else {
res = lapply(mugrid, computefn)
}
ers = sapply(res, function (robj) robj$er)
bi = which.min(ers)
regres = res[[bi]]
if(solvef1){
pi0grid = pi0grid[bi]
#robj = m1(y, x, pi0grid = pi0grid[bi], mt = mt)
robj = marg1(y, x, level = level)
} else {
robj = list(pi0grid = pi0grid, mugrid = mugrid,
pi0 = pi0grid[bi], mu = mugrid[bi],
regres = regres, res = res)
}
return(robj)
}
# solve marginal regression (logistic model) assuming mean under alternate is 1
m2boptim = function(y, x, binit, lambda = 1e-2/length(y)){
# by default this provides a l2-regularization
# equivalent to gaussian prior N(0,100) on all parameters
# x = cbind(1, x);
# defining objective function
lsobj = function(bb, y, x, lambda){
pivec = as.vector(1/(1 + exp(-x %*% bb)))
mean((y - pivec)^2) + lambda*sum(bb^2)
}
# defining first gradient
lsfirst = function(bb, y, x, lambda){
pivec = as.vector(1/(1 + exp(-x %*% bb)))
wts = -2 * (y - pivec) * pivec * (1-pivec)
apply(x, 2, function(vec) mean(vec * wts)) + 2*lambda*bb
}
optimres = optim(par = binit, fn = lsobj, gr = lsfirst,
y = y, x = x, lambda = lambda,
method = 'BFGS')
b = optimres$par
p = as.vector(1/(1 + exp(-x %*% b)))
return(list(b = b, p = p))
}
rmsepath = function(obj, dd){
rmses = numeric(length(obj$ll))
with(obj , {
for (i in 1:length(rmses)){
p = obj$lp_list[[i]]$p
f1 = obj$kw_list[[i]]$f1y
lfdr = 1 - (p * f1)/((1-p) * f0y + p * f1)
rmses[i] <<- rmse(dd$localfdr, lfdr)
}
})
return(rmses)
}
# from a full mle solution with lots of iterations, extract an earlier iteration for comparison
extract_index = function(obj, ii){
with(obj, {
rr <<- list(atoms = kw_list[[ii]]$atoms, probs = kw_list[[ii]]$probs,
f1y = kw_list[[ii]]$f1y, f0y = f0y,
b = lp_list[[ii]]$b, p = lp_list[[ii]]$p,
ll = ll[ii], rejset = rejset)
})
rr$den = with(rr, (1-p)*f0y + p * f1y)
rr$localfdr = with(rr, (1-p)*f0y/((1-p)*f0y + p * f1y))
return(rr)
}
add_alpha = function(cols, alpha = 0.7)
rgb(red = t(col2rgb(cols))/255, alpha = alpha)
rmlast = function(ss) substr(ss, 1, nchar(ss) - 1)
# find good indices
fgi = function(vec, alpha = 0.5) which(vec >= max(vec) - alpha * sd(vec))
# find index upto which vec increases
fc = function(vec, ivec = 1:length(vec)){
first_decrease = which(sign(c(diff(vec), -1)) == -1)[1]
ivec[first_decrease]
}
# relocate vec so that minimum is zero
rmmin = function(vec) {vec - min(vec)}
# mod fdrreg
modf = function(obj){
obj$b = obj$model$coef
obj$p = obj$priorprob
obj$f0y = obj$M0
obj$f1y = obj$M1
obj$den = (1-obj$p) * obj$f0y + obj$p * obj$f1y
obj$ll = mean(log(obj$den))
return(obj)
}
# extract initialization
extract_init = function(obj){
return(list(p = obj$p, b = obj$b, w = 1 - obj$localfdr))
}
npmleEM <- function(y, x, level = 0.05, initp = 1){
if(initp == 1)
{
m1n_ = marg1(y, x, level = level)
init_list = list(m1n_ = m1n_)
init_bi = which.max(sapply(init_list, function (ro) ro$ll))
init_best = extract_init(init_list[[init_bi]])
init_best_name = names(init_bi)
}
if(initp == 2)
{
m2n_ = marg2(y, x, level = level)
init_list = list(m2n_ = m2n_)
init_bi = which.max(sapply(init_list, function (ro) ro$ll))
init_best = extract_init(init_list[[init_bi]])
init_best_name = names(init_bi)
}
if(initp == 3)
{
ff=FDRreg(y,x[,-1])
f_ = modf(ff)
init_list = list(f_ = f_)
init_bi = which.max(sapply(init_list, function (ro) ro$ll))
init_best = extract_init(init_list[[init_bi]])
init_best_name = names(init_bi)
}
if(initp == 4)
{
ff=FDRreg(y,x[,-1])
f_ = modf(ff)
m1n_ = marg1(y, x, level = level)
m2n_ = marg2(y, x, level = level)
init_list = list(f_ = f_, m1n_ = m1n_, m2n_ = m2n_)
init_bi = which.max(sapply(init_list, function (ro) ro$ll))
init_best = extract_init(init_list[[init_bi]])
init_best_name = names(init_bi)
}
# EM starting from here, run at most 500 iterations
em_ = lgem(y, x, weights = init_best$w, binit = init_best$b, timed = Inf, maxit = 100, level = level)
em_ = extract_index(em_, length(em_$time_list))
return(em_)
}
st=makedata(100,cbind(runif(100),runif(100)),c(0,1,-1),c(0,1),c(0.4,0.6),c(1,1))
m1n_ = marg1(st$y, cbind(1, st$xs), level = 0.05)
m2n_ = marg2(st$y, cbind(1, st$xs), level = 0.5)
nmle_ = npmleEM(st$y, cbind(1, st$xs), level = 0.1, initp = 1)
names(nmle_)
sum(m1n_$rejset)
sum(m2n_$rejset)
m1n_$b
m2n_$b
nmle_$b
nmle_$ll
m1n_$ll
m2n_$ll
nmle_ = npmleEM(st$y, cbind(1, st$xs), level = 0.1, initp = 3)
nmle_$ll
nmle_$b
nmle_ = npmleEM(st$y, cbind(1, st$xs), level = 0.1, initp = 2)
nmle_$b
nmle_ = npmleEM(st$y, cbind(1, st$xs), level = 0.1, initp = 4)
nmle_$b
nmle_$ll
sum(nmle_$rejset)
nmle_ = npmleEM(st$y, cbind(1, st$xs), level = 0.3, initp = 4)
sum(nmle_$rejset)
require(devtools)
require(roxygen2)
devtools::document()
warnings()
system("R CMD Rd2pdf . --title=Package NPMLEmix --output=./manual.pdf --force --no-clean --internals")
devtools::document()
system("R CMD Rd2pdf . --title=Package NPMLEmix --output=./manual.pdf --force --no-clean --internals")
require(devtools)
require(roxygen2)
devtools::document()
warnings()
devtools::document()
